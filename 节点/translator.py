import torch
from .utils import get_installed_models, load_config, save_config, load_llm_model

class LLM_Translator_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        installed = get_installed_models()
        # å·²æ·»åŠ  Qwen3-4B-Instruct-2507
        presets = ["Qwen2.5-7B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-1.5B-Instruct", "Qwen3-4B-Instruct-2507"]
        all_models = sorted(list(set(installed + presets)))
        config = load_config()
        default_model = config.get("last_model", all_models[0] if all_models else "")
        
        if default_model and default_model not in all_models:
            all_models.insert(0, default_model)

        return {
            "required": {
                "æ–‡æœ¬å†…å®¹": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œä¸–ç•Œ"}),
                "æ¨¡å‹åç§°": (all_models, {"default": default_model}),
                "ç›®æ ‡è¯­è¨€": (["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "éŸ©æ–‡", "æ³•æ–‡", "å¾·æ–‡"], {"default": "ä¸­æ–‡"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "ç³»ç»ŸæŒ‡ä»¤": ("STRING", {"multiline": True, "default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚"}),
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 1024}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("ç¿»è¯‘ç»“æœ",)
    FUNCTION = "translate"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "ä½¿ç”¨æœ¬åœ°LLMæ¨¡å‹è¿›è¡Œå¤šè¯­è¨€ç¿»è¯‘ã€‚åŒ…å«è‡ªåŠ¨ä¸‹è½½æ¨¡å‹åŠŸèƒ½ã€‚"

    def translate(self, æ–‡æœ¬å†…å®¹, æ¨¡å‹åç§°, ç›®æ ‡è¯­è¨€, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, ç³»ç»ŸæŒ‡ä»¤, æœ€å¤§ç”Ÿæˆé•¿åº¦):
        save_config(æ¨¡å‹åç§°)
        
        # ç®€å•å¤„ç†ï¼šå°è¯•è‡ªåŠ¨è¡¥å…¨ repo_id ç”¨äºä¸‹è½½
        # å¦‚æœæ¨¡å‹åç§°ä¸­ä¸åŒ…å« "/" ä¸”åŒ…å« "Qwen"ï¼Œåˆ™å°è¯•åŠ ä¸Š "Qwen/" å‰ç¼€
        # æ³¨æ„ï¼šè¿™åªæ˜¯ä¸ºäº†çŒœæµ‹ä¸‹è½½è·¯å¾„ï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹ä¸åœ¨ Qwen å®˜æ–¹ä»“åº“ä¸‹ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½
        download_repo_id = æ¨¡å‹åç§°
        if è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ and "Qwen" in æ¨¡å‹åç§° and "/" not in æ¨¡å‹åç§°:
             download_repo_id = f"Qwen/{æ¨¡å‹åç§°}"

        tokenizer, model = load_llm_model(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹)
        
        lang_map = {
            "ä¸­æ–‡": "Chinese", "è‹±æ–‡": "English", "æ—¥æ–‡": "Japanese", 
            "éŸ©æ–‡": "Korean", "æ³•æ–‡": "French", "å¾·æ–‡": "German"
        }
        target_lang_en = lang_map.get(ç›®æ ‡è¯­è¨€, ç›®æ ‡è¯­è¨€)

        messages = [
            {"role": "system", "content": f"{ç³»ç»ŸæŒ‡ä»¤} Target Language: {target_lang_en}."},
            {"role": "user", "content": æ–‡æœ¬å†…å®¹}
        ]
        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text_input], return_tensors="pt").to(self.device)
        
        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=æœ€å¤§ç”Ÿæˆé•¿åº¦, pad_token_id=tokenizer.eos_token_id)
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        return (tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0],)