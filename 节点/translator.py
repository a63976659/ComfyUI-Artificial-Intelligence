import torch
from .utils import get_installed_models, load_config, save_config, load_llm_model

class LLM_Translator_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        installed = get_installed_models()
        presets = ["Qwen2.5-7B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-1.5B-Instruct", "Qwen3-4B-Instruct-2507"]
        all_models = sorted(list(set(installed + presets)))
        config = load_config()
        default_model = config.get("last_model", all_models[0] if all_models else "")
        
        if default_model and default_model not in all_models:
            all_models.insert(0, default_model)

        # å®Œæ•´çš„ç›®æ ‡è¯­è¨€åˆ—è¡¨
        target_languages = [
            "ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "éŸ©æ–‡", "æ³•æ–‡", "å¾·æ–‡",
            "è¥¿ç­ç‰™è¯­", "ä¿„è¯­", "é˜¿æ‹‰ä¼¯è¯­", "è‘¡è„ç‰™è¯­ï¼ˆåŒ…æ‹¬å·´è¥¿è‘¡è„ç‰™è¯­ï¼‰", "æ„å¤§åˆ©è¯­", 
            "æ³°è¯­", "å°åœ°è¯­", "è¶Šå—è¯­", "å°å°¼è¯­", "è·å…°è¯­", "åœŸè€³å…¶è¯­", "é˜¿å§†å“ˆæ‹‰è¯­", 
            "å¸Œè…Šè¯­", "æ³¢æ–¯è¯­ï¼ˆä¼Šæœ—è¯­ï¼‰", "é˜¿å°”å·´å°¼äºšè¯­", "ä¹Œå°”éƒ½è¯­", "å¡å°”ç»´äºšè¯­", 
            "ç«‹é™¶å®›è¯­", "èŠ¬å…°è¯­", "å†°å²›è¯­", "é©¬æ¥è¯­", "ä¿åŠ åˆ©äºšè¯­", 
            "å“¥ä¼¦æ¯”äºšè¥¿ç­ç‰™è¯­ï¼ˆç‰¹å®šæ–¹è¨€ï¼‰", "æ–°è¥¿å…°è‹±è¯­ï¼ˆå«åœ°æ–¹è¡¨è¾¾ï¼‰"
        ]

        return {
            "required": {
                "æ–‡æœ¬å†…å®¹": ("STRING", {"multiline": True, "default": "ä¸€ä¸ªå¥³å­©åœ¨é›¨ä¸­"}),
                "æ¨¡å‹åç§°": (all_models, {"default": default_model}),
                "ç›®æ ‡è¯­è¨€": (target_languages, {"default": "è‹±æ–‡"}),
                # æ–°å¢ï¼šæç¤ºè¯æ¶¦è‰²å¼€å…³
                "æç¤ºè¯æ¶¦è‰²": ("BOOLEAN", {"default": False}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 1024}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("ç¿»è¯‘ç»“æœ",)
    FUNCTION = "translate"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "ä½¿ç”¨æœ¬åœ°LLMæ¨¡å‹è¿›è¡Œå¤šè¯­è¨€ç¿»è¯‘ã€‚å¼€å¯'æç¤ºè¯æ¶¦è‰²'å¯è‡ªåŠ¨ä¸°å¯Œç»†èŠ‚ï¼Œé€‚åˆç»˜ç”»Promptç”Ÿæˆã€‚"

    def translate(self, æ–‡æœ¬å†…å®¹, æ¨¡å‹åç§°, ç›®æ ‡è¯­è¨€, æç¤ºè¯æ¶¦è‰², è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, æœ€å¤§ç”Ÿæˆé•¿åº¦):
        save_config(æ¨¡å‹åç§°)
        
        # === æ ¸å¿ƒé€»è¾‘ä¿®æ”¹ï¼šæ ¹æ®å¼€å…³åˆ‡æ¢éšè—æŒ‡ä»¤ ===
        
        # æŒ‡ä»¤ 1: çº¯å‡€ç¿»è¯‘ (å…³é—­æ¶¦è‰²æ—¶ä½¿ç”¨)
        instruction_1 = "You are a professional translator. Translate the following text directly without explanation."
        
        # æŒ‡ä»¤ 2: ç¿»è¯‘ + æ¶¦è‰²/ç¾åŒ– (å¼€å¯æ¶¦è‰²æ—¶ä½¿ç”¨)
        instruction_2 = (
            "You are a professional prompt engineer and translator. Your task is to translate the user's input into the target language. "
            "CRITICAL: You must also refine, beautify, and add descriptive details (lighting, texture, atmosphere, composition) "
            "to make the text vivid and high-quality, suitable for AI art generation. "
            "Output ONLY the final result without explanation or conversational filler."
        )

        # æ ¹æ®å¼€å…³é€‰æ‹©æŒ‡ä»¤
        if æç¤ºè¯æ¶¦è‰²:
            system_instruction = instruction_2
        else:
            system_instruction = instruction_1

        # ==========================================

        # ç®€å•å¤„ç†ä¸‹è½½è·¯å¾„çŒœæµ‹
        download_repo_id = æ¨¡å‹åç§°
        if è‡ªåŠ¨ä¸‹è½½æ¨¡å‹ and "Qwen" in æ¨¡å‹åç§° and "/" not in æ¨¡å‹åç§°:
             download_repo_id = f"Qwen/{æ¨¡å‹åç§°}"

        tokenizer, model = load_llm_model(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹)
        
        # è¯­è¨€æ˜ å°„å­—å…¸
        lang_map = {
            "ä¸­æ–‡": "Chinese", "è‹±æ–‡": "English", "æ—¥æ–‡": "Japanese", 
            "éŸ©æ–‡": "Korean", "æ³•æ–‡": "French", "å¾·æ–‡": "German",
            "è¥¿ç­ç‰™è¯­": "Spanish", "ä¿„è¯­": "Russian", "é˜¿æ‹‰ä¼¯è¯­": "Arabic", 
            "è‘¡è„ç‰™è¯­ï¼ˆåŒ…æ‹¬å·´è¥¿è‘¡è„ç‰™è¯­ï¼‰": "Portuguese (including Brazilian Portuguese)", 
            "æ„å¤§åˆ©è¯­": "Italian", "æ³°è¯­": "Thai", "å°åœ°è¯­": "Hindi", 
            "è¶Šå—è¯­": "Vietnamese", "å°å°¼è¯­": "Indonesian", "è·å…°è¯­": "Dutch", 
            "åœŸè€³å…¶è¯­": "Turkish", "é˜¿å§†å“ˆæ‹‰è¯­": "Amharic", "å¸Œè…Šè¯­": "Greek", 
            "æ³¢æ–¯è¯­ï¼ˆä¼Šæœ—è¯­ï¼‰": "Persian (Farsi)", "é˜¿å°”å·´å°¼äºšè¯­": "Albanian", 
            "ä¹Œå°”éƒ½è¯­": "Urdu", "å¡å°”ç»´äºšè¯­": "Serbian", "ç«‹é™¶å®›è¯­": "Lithuanian", 
            "èŠ¬å…°è¯­": "Finnish", "å†°å²›è¯­": "Icelandic", "é©¬æ¥è¯­": "Malay", 
            "ä¿åŠ åˆ©äºšè¯­": "Bulgarian", 
            "å“¥ä¼¦æ¯”äºšè¥¿ç­ç‰™è¯­ï¼ˆç‰¹å®šæ–¹è¨€ï¼‰": "Colombian Spanish (Specific Dialect)", 
            "æ–°è¥¿å…°è‹±è¯­ï¼ˆå«åœ°æ–¹è¡¨è¾¾ï¼‰": "New Zealand English (Including local expressions)"
        }
        target_lang_en = lang_map.get(ç›®æ ‡è¯­è¨€, ç›®æ ‡è¯­è¨€)

        messages = [
            {"role": "system", "content": f"{system_instruction} Target Language: {target_lang_en}."},
            {"role": "user", "content": æ–‡æœ¬å†…å®¹}
        ]
        text_input = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        model_inputs = tokenizer([text_input], return_tensors="pt").to(self.device)
        
        generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=æœ€å¤§ç”Ÿæˆé•¿åº¦, pad_token_id=tokenizer.eos_token_id)
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        return (tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0],)