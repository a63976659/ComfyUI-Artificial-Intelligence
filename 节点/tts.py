import torch
import numpy as np
import random
import re
import torchaudio
from .utils import load_tts_model_data

# ================= æ˜ å°„å­—å…¸ =================
SPEAKER_MAPPING = {
    "Vivian (ä¸­æ–‡-æ˜äº®å¾®æ€¥)": "Vivian",
    "Serena (ä¸­æ–‡-æ¸©æš–æ¸©æŸ”)": "Serena",
    "Uncle_Fu (ä¸­æ–‡-é†‡åšç”·å£°)": "Uncle_Fu",
    "Dylan (ä¸­æ–‡-åŒ—äº¬å°‘å¹´)": "Dylan",
    "Eric (ä¸­æ–‡-å››å·è¯)": "Eric",
    "Ryan (è‹±æ–‡-åŠ¨æ„ŸèŠ‚å¥)": "Ryan",
    "Aiden (è‹±æ–‡-é˜³å…‰ç”·å£°)": "Aiden",
    "Ono_Anna (æ—¥æ–‡-ä¿çš®çµåŠ¨)": "Ono_Anna",
    "Sohee (éŸ©æ–‡-æ¸©æš–æƒ…æ„Ÿ)": "Sohee"
}

LANGUAGE_MAPPING = {
    "è‡ªåŠ¨è¯†åˆ« (Auto)": "Auto",
    "ä¸­æ–‡ (Chinese)": "Chinese",
    "è‹±æ–‡ (English)": "English",
    "æ—¥æ–‡ (Japanese)": "Japanese",
    "éŸ©æ–‡ (Korean)": "Korean",
    "å¾·æ–‡ (German)": "German",
    "æ³•æ–‡ (French)": "French",
    "ä¿„æ–‡ (Russian)": "Russian",
    "è‘¡è„ç‰™æ–‡ (Portuguese)": "Portuguese",
    "è¥¿ç­ç‰™æ–‡ (Spanish)": "Spanish",
    "æ„å¤§åˆ©æ–‡ (Italian)": "Italian"
}

# ================= é€šç”¨è¾…åŠ©å‡½æ•° =================
def _parse_text_with_pauses(text_input):
    input_lines = [t.strip() for t in text_input.split("\n") if t.strip()]
    segments = []
    pause_pattern = re.compile(r"\[(?:pause|p):(\d+(?:\.\d+)?)\]", re.IGNORECASE)

    for line in input_lines:
        last_idx = 0
        for match in pause_pattern.finditer(line):
            text_part = line[last_idx : match.start()].strip()
            if text_part:
                segments.append(("text", text_part))
            try:
                duration = float(match.group(1))
                segments.append(("pause", duration))
            except ValueError:
                pass
            last_idx = match.end()

        remaining_text = line[last_idx:].strip()
        if remaining_text:
            segments.append(("text", remaining_text))
    return segments

def _set_seed(seed):
    if seed is not None:
        seed = seed & 0xffffffff
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)
        np.random.seed(seed)
        random.seed(seed)

def _process_ref_audio(audio_dict):
    """å¤„ç†å‚è€ƒéŸ³é¢‘ï¼šè½¬æ¢ä¸ºå•å£°é“å¹¶é‡é‡‡æ ·åˆ° 16k"""
    waveform = audio_dict['waveform'] 
    sr = audio_dict['sample_rate']
    
    if waveform.dim() == 3:
        waveform = waveform[0] 
    
    if waveform.shape[0] > waveform.shape[1] and waveform.shape[0] > 100:
        waveform = waveform.t()

    if waveform.shape[0] > 1:
        waveform = torch.mean(waveform, dim=0, keepdim=True)
    
    target_sr = 16000
    if sr != target_sr:
        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=target_sr)
        waveform = resampler(waveform)
    
    wav_numpy = waveform.squeeze().cpu().numpy()
    return wav_numpy, target_sr

def _process_output(audio_segments_np, sr, output_mode):
    """ç»Ÿä¸€å¤„ç†è¾“å‡ºæ¨¡å¼ï¼šæ‹¼åˆ æˆ– æ‰¹æ¬¡"""
    if not audio_segments_np:
        raise Exception("æœªç”ŸæˆéŸ³é¢‘")

    if output_mode == "æ‹¼åˆ (Concatenate)":
        full_audio = np.concatenate(audio_segments_np)
        audio_tensor = torch.from_numpy(full_audio).float()
        # (1, 1, Samples)
        if audio_tensor.dim() == 1:
            audio_tensor = audio_tensor.unsqueeze(0).unsqueeze(0)
        return ({"waveform": audio_tensor, "sample_rate": sr},)
    
    else: # æ‰¹æ¬¡ (Batch)
        max_len = max(len(seg) for seg in audio_segments_np)
        batch_size = len(audio_segments_np)
        batch_tensor = torch.zeros(batch_size, 1, max_len, dtype=torch.float32)
        
        for i, seg in enumerate(audio_segments_np):
            tensor_seg = torch.from_numpy(seg).float()
            length = tensor_seg.shape[0]
            batch_tensor[i, 0, :length] = tensor_seg
            
        return ({"waveform": batch_tensor, "sample_rate": sr},)

# ================= èŠ‚ç‚¹ 1: CustomVoice (é¢„è®¾è§’è‰²) =================
class Qwen_TTS_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        presets = ["Qwen3-TTS-12Hz-1.7B-CustomVoice", "Qwen3-TTS-12Hz-0.6B-CustomVoice"]
        return {
            "required": {
                "æ–‡æœ¬å†…å®¹": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œæˆ‘æ˜¯Vivianã€‚"}),
                "æ¨¡å‹åç§°": (presets, {"default": presets[0]}),
                "è¯­è¨€": (list(LANGUAGE_MAPPING.keys()), {"default": "è‡ªåŠ¨è¯†åˆ« (Auto)"}),
                "è¯´è¯äºº": (list(SPEAKER_MAPPING.keys()), {"default": "Vivian (ä¸­æ–‡-æ˜äº®å¾®æ€¥)"}),
                "æƒ…æ„ŸæŒ‡ä»¤": ("STRING", {"multiline": False, "default": "é«˜å…´", "placeholder": "ä¾‹å¦‚ï¼šé«˜å…´ã€æ‚²ä¼¤"}),
                
                # æ”¹å›æ ‡å‡†åç§° 'seed' ä»¥å¯ç”¨ ComfyUI çš„è‡ªåŠ¨æ§åˆ¶ç»„ä»¶ (Fixed/Increment/Randomize)
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffff}),
                
                # --- ä¸»ç”Ÿæˆå‚æ•° ---
                "æ¸©åº¦": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 2.0, "step": 0.1}),
                "Top_P": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 1.0, "step": 0.05}),
                "Top_K": ("INT", {"default": 50, "min": 0, "max": 100}),
                "é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.1, "min": 0.1, "max": 2.0, "step": 0.05}),
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 2048, "min": 128, "max": 8192}),
                
                "è¾“å‡ºæ¨¡å¼": (["æ‹¼åˆ (Concatenate)", "æ‰¹æ¬¡ (Batch)"], {"default": "æ‹¼åˆ (Concatenate)"}),
                
                # --- ä¸‹è½½ç›¸å…³ (æ”¾åœ¨æœ€å) ---
                "ä¸‹è½½æº": (["ModelScope", "HuggingFace", "HF Mirror"], {"default": "ModelScope"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("éŸ³é¢‘è¾“å‡º",)
    FUNCTION = "generate_speech"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "ã€é¢„è®¾è§’è‰²æ¨¡å¼ã€‘\nåŸºäºå®˜æ–¹9ä½é¢„è®¾è§’è‰²ã€‚æ”¯æŒæƒ…æ„ŸæŒ‡ä»¤æ§åˆ¶ã€‚"

    def generate_speech(self, æ–‡æœ¬å†…å®¹, æ¨¡å‹åç§°, è¯­è¨€, è¯´è¯äºº, æƒ…æ„ŸæŒ‡ä»¤, seed, æ¸©åº¦, Top_P, Top_K, é‡å¤æƒ©ç½š, æœ€å¤§ç”Ÿæˆé•¿åº¦, è¾“å‡ºæ¨¡å¼, ä¸‹è½½æº, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹):
        _set_seed(seed)
        model = load_tts_model_data(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, source=ä¸‹è½½æº)

        try:
            target_speaker = SPEAKER_MAPPING.get(è¯´è¯äºº, "Vivian")
            target_language = LANGUAGE_MAPPING.get(è¯­è¨€, "Auto")
            gen_kwargs = {
                "temperature": æ¸©åº¦, "top_p": Top_P, "top_k": Top_K,
                "repetition_penalty": é‡å¤æƒ©ç½š, "max_new_tokens": æœ€å¤§ç”Ÿæˆé•¿åº¦,
                "subtalker_dosample": True 
            }
            
            segments = _parse_text_with_pauses(æ–‡æœ¬å†…å®¹)
            if not segments: raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
            
            audio_segments_np = []
            sr = 24000 

            for seg_type, content in segments:
                if seg_type == "pause":
                    if content > 0: audio_segments_np.append(np.zeros(int(content * sr), dtype=np.float32))
                else:
                    instruct_text = æƒ…æ„ŸæŒ‡ä»¤.strip() if æƒ…æ„ŸæŒ‡ä»¤.strip() else None
                    wavs, current_sr = model.generate_custom_voice(
                        text=[content], language=[target_language], speaker=[target_speaker],
                        instruct=[instruct_text] if instruct_text else None, **gen_kwargs
                    )
                    sr = current_sr
                    if len(wavs) > 0: audio_segments_np.append(wavs[0].squeeze() if wavs[0].ndim > 1 else wavs[0])

            return _process_output(audio_segments_np, sr, è¾“å‡ºæ¨¡å¼)

        except Exception as e:
            raise Exception(f"CustomVoice ç”Ÿæˆå¤±è´¥: {str(e)}")

# ================= èŠ‚ç‚¹ 2: VoiceDesign (æ–‡æœ¬æéŸ³) =================
class Qwen_TTS_VoiceDesign_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        presets = ["Qwen3-TTS-12Hz-1.7B-VoiceDesign"]
        return {
            "required": {
                "æ–‡æœ¬å†…å®¹": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œè¿™æ˜¯ä¸€æ®µæµ‹è¯•è¯­éŸ³ã€‚"}),
                "æ¨¡å‹åç§°": (presets, {"default": presets[0]}),
                "è¯­è¨€": (list(LANGUAGE_MAPPING.keys()), {"default": "è‡ªåŠ¨è¯†åˆ« (Auto)"}),
                "å£°éŸ³è®¾è®¡æè¿°": ("STRING", {"multiline": False, "default": "ä½“ç°æ’’å¨‡ç¨šå«©çš„èè‰å¥³å£°ï¼ŒéŸ³è°ƒåé«˜ä¸”èµ·ä¼æ˜æ˜¾ã€‚", "placeholder": "æè¿°å£°éŸ³ç‰¹å¾ã€æ€§åˆ«ã€å¹´é¾„"}),
                
                # æ”¹å›æ ‡å‡†åç§° 'seed'
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffff}),
                
                # --- ä¸»ç”Ÿæˆå‚æ•° ---
                "æ¸©åº¦": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 2.0, "step": 0.1}),
                "Top_P": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 1.0, "step": 0.05}),
                "Top_K": ("INT", {"default": 50, "min": 0, "max": 100}),
                "é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.1, "min": 0.1, "max": 2.0, "step": 0.05}),
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 2048, "min": 128, "max": 8192}),
                
                "è¾“å‡ºæ¨¡å¼": (["æ‹¼åˆ (Concatenate)", "æ‰¹æ¬¡ (Batch)"], {"default": "æ‹¼åˆ (Concatenate)"}),
                
                # --- ä¸‹è½½ç›¸å…³ (æ”¾åœ¨æœ€å) ---
                "ä¸‹è½½æº": (["ModelScope", "HuggingFace", "HF Mirror"], {"default": "ModelScope"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("éŸ³é¢‘è¾“å‡º",)
    FUNCTION = "generate_voice_design"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "ã€æ–‡æœ¬æéŸ³æ¨¡å¼ã€‘\né€šè¿‡æ–‡å­—æè¿°åˆ›é€ å£°éŸ³ã€‚"

    def generate_voice_design(self, æ–‡æœ¬å†…å®¹, æ¨¡å‹åç§°, è¯­è¨€, å£°éŸ³è®¾è®¡æè¿°, seed, æ¸©åº¦, Top_P, Top_K, é‡å¤æƒ©ç½š, æœ€å¤§ç”Ÿæˆé•¿åº¦, è¾“å‡ºæ¨¡å¼, ä¸‹è½½æº, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹):
        _set_seed(seed)
        model = load_tts_model_data(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, source=ä¸‹è½½æº)

        try:
            target_language = LANGUAGE_MAPPING.get(è¯­è¨€, "Auto")
            gen_kwargs = {
                "temperature": æ¸©åº¦, "top_p": Top_P, "top_k": Top_K,
                "repetition_penalty": é‡å¤æƒ©ç½š, "max_new_tokens": æœ€å¤§ç”Ÿæˆé•¿åº¦,
                "subtalker_dosample": True
            }
            segments = _parse_text_with_pauses(æ–‡æœ¬å†…å®¹)
            if not segments: raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
            if not å£°éŸ³è®¾è®¡æè¿°.strip(): raise ValueError("å£°éŸ³è®¾è®¡æè¿°ä¸èƒ½ä¸ºç©º")
            
            audio_segments_np = []
            sr = 24000 
            for seg_type, content in segments:
                if seg_type == "pause":
                    if content > 0: audio_segments_np.append(np.zeros(int(content * sr), dtype=np.float32))
                else:
                    wavs, current_sr = model.generate_voice_design(
                        text=[content], language=[target_language], 
                        instruct=[å£°éŸ³è®¾è®¡æè¿°.strip()], **gen_kwargs
                    )
                    sr = current_sr
                    if len(wavs) > 0: audio_segments_np.append(wavs[0].squeeze() if wavs[0].ndim > 1 else wavs[0])

            return _process_output(audio_segments_np, sr, è¾“å‡ºæ¨¡å¼)

        except Exception as e:
            raise Exception(f"VoiceDesign ç”Ÿæˆå¤±è´¥: {str(e)}")

# ================= èŠ‚ç‚¹ 3: VoiceClone (è¯­éŸ³å…‹éš†) =================
class Qwen_TTS_VoiceClone_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        presets = ["Qwen3-TTS-12Hz-1.7B-Base", "Qwen3-TTS-12Hz-0.6B-Base"]
        return {
            "required": {
                "å‚è€ƒéŸ³é¢‘": ("AUDIO", ),
                "æ–‡æœ¬å†…å®¹": ("STRING", {"multiline": True, "default": "é€šè¿‡å…‹éš†ä½ çš„å£°éŸ³ï¼Œæˆ‘è¯´å‡ºäº†è¿™å¥è¯ã€‚"}),
                "æ¨¡å‹åç§°": (presets, {"default": presets[0]}),
                "è¯­è¨€": (list(LANGUAGE_MAPPING.keys()), {"default": "è‡ªåŠ¨è¯†åˆ« (Auto)"}),
                
                # --- æ–°å¢é¡ºåºè°ƒæ•´ï¼šç´§è·Ÿè¯­è¨€ç»„ä»¶ ---
                "å‚è€ƒéŸ³é¢‘æ–‡æœ¬": ("STRING", {"multiline": True, "default": "", "placeholder": "(å¯é€‰) è¾“å…¥å‚è€ƒéŸ³é¢‘çš„æ–‡å­—å†…å®¹ã€‚è‹¥ç•™ç©ºåˆ™å¼ºåˆ¶ä½¿ç”¨æé€Ÿæ¨¡å¼ã€‚"}),
                "æƒ…æ„ŸæŒ‡ä»¤": ("STRING", {"multiline": False, "default": "", "placeholder": "(å¯é€‰) ä¾‹å¦‚ï¼šæ‚²ä¼¤ã€å¼€å¿ƒ"}),
                "æé€Ÿæ¨¡å¼": ("BOOLEAN", {"default": False, "label": "æé€Ÿæ¨¡å¼ (å¿½ç•¥å‚è€ƒæ–‡æœ¬)"}),
                
                # æ”¹å›æ ‡å‡†åç§° 'seed'
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffff}),
                
                # --- ä¸»ç”Ÿæˆå‚æ•° ---
                "æ¸©åº¦": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 2.0, "step": 0.1}),
                "Top_P": ("FLOAT", {"default": 0.8, "min": 0.1, "max": 1.0, "step": 0.05}),
                "Top_K": ("INT", {"default": 50, "min": 0, "max": 100}),
                "é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.1, "min": 0.1, "max": 2.0, "step": 0.05}),
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 2048, "min": 128, "max": 8192}),
                
                "è¾“å‡ºæ¨¡å¼": (["æ‹¼åˆ (Concatenate)", "æ‰¹æ¬¡ (Batch)"], {"default": "æ‹¼åˆ (Concatenate)"}),
                
                # --- å­ç”Ÿæˆå™¨å‚æ•° (ä»…å…‹éš†èŠ‚ç‚¹ä¿ç•™) ---
                "å­ç”Ÿæˆå™¨_æ¸©åº¦": ("FLOAT", {"default": 0.9, "min": 0.1, "max": 2.0, "step": 0.05}),
                "å­ç”Ÿæˆå™¨_Top_P": ("FLOAT", {"default": 1.0, "min": 0.1, "max": 1.0, "step": 0.05}),
                "å­ç”Ÿæˆå™¨_Top_K": ("INT", {"default": 50, "min": 0, "max": 100}),
                
                # --- ä¸‹è½½ç›¸å…³ (æ”¾åœ¨æœ€å) ---
                "ä¸‹è½½æº": (["ModelScope", "HuggingFace", "HF Mirror"], {"default": "ModelScope"}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("AUDIO",)
    RETURN_NAMES = ("éŸ³é¢‘è¾“å‡º",)
    FUNCTION = "generate_voice_clone"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "ã€è¯­éŸ³å…‹éš†æ¨¡å¼ã€‘\nè‹¥'å‚è€ƒéŸ³é¢‘æ–‡æœ¬'ä¸ºç©ºï¼Œå°†è‡ªåŠ¨å¼ºåˆ¶å¯ç”¨æé€Ÿæ¨¡å¼ä»¥é¿å…æŠ¥é”™ã€‚"

    def generate_voice_clone(self, å‚è€ƒéŸ³é¢‘, æ–‡æœ¬å†…å®¹, æ¨¡å‹åç§°, è¯­è¨€, seed, æ¸©åº¦, Top_P, Top_K, é‡å¤æƒ©ç½š, æœ€å¤§ç”Ÿæˆé•¿åº¦, è¾“å‡ºæ¨¡å¼, ä¸‹è½½æº, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, å­ç”Ÿæˆå™¨_æ¸©åº¦, å­ç”Ÿæˆå™¨_Top_P, å­ç”Ÿæˆå™¨_Top_K,
                             å‚è€ƒéŸ³é¢‘æ–‡æœ¬="", æƒ…æ„ŸæŒ‡ä»¤="", æé€Ÿæ¨¡å¼=False):
        _set_seed(seed)
        model = load_tts_model_data(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, source=ä¸‹è½½æº)

        try:
            target_language = LANGUAGE_MAPPING.get(è¯­è¨€, "Auto")
            gen_kwargs = {
                "temperature": æ¸©åº¦, "top_p": Top_P, "top_k": Top_K,
                "repetition_penalty": é‡å¤æƒ©ç½š, "max_new_tokens": æœ€å¤§ç”Ÿæˆé•¿åº¦,
                "subtalker_temperature": å­ç”Ÿæˆå™¨_æ¸©åº¦,
                "subtalker_top_p": å­ç”Ÿæˆå™¨_Top_P,
                "subtalker_top_k": å­ç”Ÿæˆå™¨_Top_K,
                "subtalker_dosample": True
            }
            
            # 1. å¤„ç†å‚è€ƒéŸ³é¢‘
            ref_wav_np, ref_sr = _process_ref_audio(å‚è€ƒéŸ³é¢‘)
            
            # 2. é€»è¾‘ä¿®æ­£ï¼šå¦‚æœå‚è€ƒæ–‡æœ¬ä¸ºç©ºï¼Œå¼ºåˆ¶ä½¿ç”¨æé€Ÿæ¨¡å¼
            clean_ref_text = å‚è€ƒéŸ³é¢‘æ–‡æœ¬.strip()
            
            # å†³å®šæœ€ç»ˆæ¨¡å¼
            final_x_vector_mode = æé€Ÿæ¨¡å¼
            ref_text_arg = None
            
            if not clean_ref_text:
                if not final_x_vector_mode:
                    print("[Qwen TTS Warning] æœªå¡«å†™å‚è€ƒéŸ³é¢‘æ–‡æœ¬ï¼Œå·²è‡ªåŠ¨åˆ‡æ¢è‡³ 'æé€Ÿæ¨¡å¼' (X-Vector Only)ã€‚")
                    final_x_vector_mode = True
                ref_text_arg = None
            else:
                # åªæœ‰åœ¨å…³é—­æé€Ÿæ¨¡å¼ä¸”æœ‰æ–‡æœ¬æ—¶ï¼Œæ‰ä¼ å…¥æ–‡æœ¬
                if not final_x_vector_mode:
                    ref_text_arg = clean_ref_text

            print(f"[Qwen Clone] Extracting features... Mode={'X-Vector(Fast)' if final_x_vector_mode else 'ICL(Quality)'}")
            
            voice_prompt = model.create_voice_clone_prompt(
                ref_audio=(ref_wav_np, ref_sr),
                ref_text=ref_text_arg,
                x_vector_only_mode=final_x_vector_mode
            )

            # 3. ç”ŸæˆéŸ³é¢‘
            segments = _parse_text_with_pauses(æ–‡æœ¬å†…å®¹)
            if not segments: raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
            
            audio_segments_np = []
            sr = 24000 
            for seg_type, content in segments:
                if seg_type == "pause":
                    if content > 0: audio_segments_np.append(np.zeros(int(content * sr), dtype=np.float32))
                else:
                    instruct_text = æƒ…æ„ŸæŒ‡ä»¤.strip() if æƒ…æ„ŸæŒ‡ä»¤ and æƒ…æ„ŸæŒ‡ä»¤.strip() else None
                    wavs, current_sr = model.generate_voice_clone(
                        text=[content],
                        language=[target_language],
                        voice_clone_prompt=voice_prompt,
                        instruct=[instruct_text] if instruct_text else None,
                        **gen_kwargs
                    )
                    sr = current_sr
                    if len(wavs) > 0: audio_segments_np.append(wavs[0].squeeze() if wavs[0].ndim > 1 else wavs[0])

            return _process_output(audio_segments_np, sr, è¾“å‡ºæ¨¡å¼)

        except Exception as e:
            raise Exception(f"VoiceClone ç”Ÿæˆå¤±è´¥: {str(e)}")