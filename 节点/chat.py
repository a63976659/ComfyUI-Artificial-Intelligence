import torch
import random
import numpy as np
from .utils import get_installed_models, load_config, save_config, load_llm_model

class LLM_Chat_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        installed = get_installed_models()
        # å·²æ·»åŠ  Qwen3-4B-Instruct-2507
        presets = ["Qwen2.5-7B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-1.5B-Instruct", "Qwen3-4B-Instruct-2507"]
        all_models = sorted(list(set(installed + presets)))
        config = load_config()
        default_model = config.get("last_model", all_models[0] if all_models else "")
        if default_model and default_model not in all_models:
            all_models.insert(0, default_model)

        return {
            "required": {
                "æç¤ºè¯": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚", "placeholder": "åœ¨æ­¤è¾“å…¥å¯¹è¯å†…å®¹..."}),
                "æ¨¡å‹åç§°": (all_models, {"default": default_model}),
                "éšæœºç§å­": ("INT", {"default": 0, "min": 0, "max": 0xffffffffffffffff}), 
                "æ¸©åº¦_åˆ›é€ æ€§": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 2.0, "step": 0.1}), 
                "Top_P_é‡‡æ ·ç‡": ("FLOAT", {"default": 0.9, "min": 0.1, "max": 1.0, "step": 0.05}),
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 2048, "min": 64, "max": 8192}),
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "ç³»ç»ŸæŒ‡ä»¤": ("STRING", {"multiline": True, "default": "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("å›å¤å†…å®¹",)
    FUNCTION = "chat"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "åŸºäºæœ¬åœ°å¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹ã€‚æ”¯æŒéšæœºç§å­æ§åˆ¶ã€æ¸©åº¦è°ƒæ•´å’Œè‡ªåŠ¨æ¨¡å‹ä¸‹è½½ã€‚"

    def chat(self, æç¤ºè¯, æ¨¡å‹åç§°, éšæœºç§å­, æ¸©åº¦_åˆ›é€ æ€§, Top_P_é‡‡æ ·ç‡, æœ€å¤§ç”Ÿæˆé•¿åº¦, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹, ç³»ç»ŸæŒ‡ä»¤):
        # 1. ä¿å­˜é…ç½®
        save_config(æ¨¡å‹åç§°)

        # 2. è®¾ç½®éšæœºç§å­
        if éšæœºç§å­ is not None:
            torch.manual_seed(éšæœºç§å­)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(éšæœºç§å­)
            np.random.seed(éšæœºç§å­)
            random.seed(éšæœºç§å­)

        # 3. åŠ è½½æ¨¡å‹
        tokenizer, model = load_llm_model(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹)

        # 4. æ„å»ºå¯¹è¯
        messages = [
            {"role": "system", "content": ç³»ç»ŸæŒ‡ä»¤},
            {"role": "user", "content": æç¤ºè¯}
        ]
        
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # 5. æ¨ç†
        model_inputs = tokenizer([text_input], return_tensors="pt").to(self.device)
        
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=æœ€å¤§ç”Ÿæˆé•¿åº¦,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=æ¸©åº¦_åˆ›é€ æ€§,
            top_p=Top_P_é‡‡æ ·ç‡
        )
        
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        return (response,)