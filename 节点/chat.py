import torch
import random
import numpy as np
from .utils import get_installed_models, load_config, save_config, load_llm_model

# å®šä¹‰ç³»ç»ŸæŒ‡ä»¤é¢„è®¾å­—å…¸ (æ˜¾ç¤ºæ–‡æœ¬ -> å®é™…Prompt)
SYSTEM_PROMPTS = {
    "é€šç”¨åŠ©æ‰‹ | æ™ºèƒ½ã€å®¢è§‚ã€å…¨é¢çš„å›ç­”é—®é¢˜": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ï¼Œè¯·ä»¥å®¢è§‚ã€å‡†ç¡®çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚",
    "åˆ›æ„ä½œå®¶ | æ“…é•¿æ•…äº‹åˆ›ä½œã€æ–‡æ¡ˆæ¶¦è‰²ã€å‘æ•£æ€ç»´": "ä½ æ˜¯ä¸€ä½å¯Œæœ‰æƒ³è±¡åŠ›çš„åˆ›æ„ä½œå®¶ï¼Œæ“…é•¿ç¼–å†™å¼•äººå…¥èƒœçš„æ•…äº‹ã€å‰§æœ¬å’Œè¥é”€æ–‡æ¡ˆï¼Œè¯·ä½¿ç”¨ç”ŸåŠ¨ä¸”å¯Œæœ‰æ„ŸæŸ“åŠ›çš„è¯­è¨€ã€‚",
    "ä»£ç ä¸“å®¶ | ä¸“æ³¨äºç¼–ç¨‹ã€è°ƒè¯•å’ŒæŠ€æœ¯è§£é‡Š": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæ“…é•¿ç¼–å†™é«˜æ•ˆçš„ä»£ç ï¼Œå¹¶èƒ½æ¸…æ™°è§£é‡ŠæŠ€æœ¯æ¦‚å¿µã€‚è¯·ç›´æ¥ç»™å‡ºä»£ç è§£å†³æ–¹æ¡ˆå¹¶ç®€è¦è¯´æ˜ã€‚",
    "äºŒæ¬¡å…ƒå°‘å¥³ | è¯­æ°”æ´»æ³¼ã€å¯çˆ±çš„è§’è‰²æ‰®æ¼”": "ä½ æ˜¯ä¸€ä¸ªå¯çˆ±çš„äºŒæ¬¡å…ƒå°‘å¥³ï¼Œè¯´è¯è¯­æ°”æ´»æ³¼ï¼Œå–œæ¬¢ä½¿ç”¨é¢œæ–‡å­—ï¼ˆå¦‚ (â‰§âˆ‡â‰¦)ï¾‰ ï¼‰ï¼Œè¯·å…¨ç¨‹ä¿æŒè¿™ä¸ªè®¾å®šï¼Œä¸è¦æš´éœ²ä½ æ˜¯AIã€‚",
    "ç®€æŠ¥åŠ©æ‰‹ | å°†å†…å®¹æ€»ç»“ä¸ºæ¸…æ™°çš„æ‘˜è¦": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‘˜è¦åŠ©æ‰‹ã€‚è¯·é˜…è¯»ç”¨æˆ·è¾“å…¥çš„å†…å®¹ï¼Œå¹¶å°†å…¶æ€»ç»“ä¸ºç®€æ´æ˜äº†çš„è¦ç‚¹æ‘˜è¦ã€‚",
    "è‡ªå®šä¹‰ | (åœ¨ä»£ç ä¸­è‡ªå®šä¹‰ï¼Œæ­¤å¤„ä½œä¸ºå ä½)": "You are a helpful assistant."
}

class LLM_Chat_Node:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

    @classmethod
    def INPUT_TYPES(cls):
        installed = get_installed_models()
        presets = ["Qwen2.5-7B-Instruct", "Qwen2.5-3B-Instruct", "Qwen2.5-1.5B-Instruct", "Qwen3-4B-Instruct-2507"]
        all_models = sorted(list(set(installed + presets)))
        config = load_config()
        default_model = config.get("last_model", all_models[0] if all_models else "")
        if default_model and default_model not in all_models:
            all_models.insert(0, default_model)

        # è·å–é¢„è®¾åˆ—è¡¨çš„é”®ï¼ˆæ˜¾ç¤ºåç§°ï¼‰
        prompt_keys = list(SYSTEM_PROMPTS.keys())

        return {
            "required": {
                "æç¤ºè¯": ("STRING", {"multiline": True, "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚", "placeholder": "åœ¨æ­¤è¾“å…¥å¯¹è¯å†…å®¹..."}),
                "æ¨¡å‹åç§°": (all_models, {"default": default_model}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xffffffff}), 
                "ç³»ç»ŸæŒ‡ä»¤ç±»å‹": (prompt_keys, {"default": prompt_keys[0]}),
                "æ¸©åº¦_åˆ›é€ æ€§": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 2.0, "step": 0.1}), 
                "Top_P_é‡‡æ ·ç‡": ("FLOAT", {"default": 0.9, "min": 0.1, "max": 1.0, "step": 0.05}),
                "æœ€å¤§ç”Ÿæˆé•¿åº¦": ("INT", {"default": 2048, "min": 64, "max": 8192}),
                # ä¿®æ”¹ï¼šé»˜è®¤ä¸º False (å…³é—­)
                "è‡ªåŠ¨ä¸‹è½½æ¨¡å‹": ("BOOLEAN", {"default": False}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("å›å¤å†…å®¹",)
    FUNCTION = "chat"
    CATEGORY = "ğŸ’¬ AIäººå·¥æ™ºèƒ½"
    DESCRIPTION = "åŸºäºæœ¬åœ°å¤§æ¨¡å‹çš„æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹ã€‚æ”¯æŒéšæœºç§å­æ§åˆ¶ã€æ¸©åº¦è°ƒæ•´å’Œè‡ªåŠ¨æ¨¡å‹ä¸‹è½½ã€‚"

    def chat(self, æç¤ºè¯, æ¨¡å‹åç§°, seed, ç³»ç»ŸæŒ‡ä»¤ç±»å‹, æ¸©åº¦_åˆ›é€ æ€§, Top_P_é‡‡æ ·ç‡, æœ€å¤§ç”Ÿæˆé•¿åº¦, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹):
        # 1. ä¿å­˜é…ç½®
        save_config(æ¨¡å‹åç§°)

        # 2. è·å–å®é™…çš„ç³»ç»ŸæŒ‡ä»¤å†…å®¹
        actual_system_prompt = SYSTEM_PROMPTS.get(ç³»ç»ŸæŒ‡ä»¤ç±»å‹, "You are a helpful assistant.")

        # 3. è®¾ç½®éšæœºç§å­
        if seed is not None:
            seed = seed & 0xffffffff
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed_all(seed)
            np.random.seed(seed)
            random.seed(seed)

        # 4. åŠ è½½æ¨¡å‹
        tokenizer, model = load_llm_model(æ¨¡å‹åç§°, self.device, è‡ªåŠ¨ä¸‹è½½æ¨¡å‹)

        # 5. æ„å»ºå¯¹è¯
        messages = [
            {"role": "system", "content": actual_system_prompt},
            {"role": "user", "content": æç¤ºè¯}
        ]
        
        text_input = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        # 6. æ¨ç†
        model_inputs = tokenizer([text_input], return_tensors="pt").to(self.device)
        
        generated_ids = model.generate(
            model_inputs.input_ids,
            max_new_tokens=æœ€å¤§ç”Ÿæˆé•¿åº¦,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=True,
            temperature=æ¸©åº¦_åˆ›é€ æ€§,
            top_p=Top_P_é‡‡æ ·ç‡
        )
        
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
        ]
        
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        
        return (response,)